---
title: "One-Way Analysis of Variance"
#author: "Burrel Vann Jr"
output: 
  html_document: 
    css: markdown3.css
---
<br>

###### Getting RStudio Up and Running
* [**Installing R/RStudio**](installing.html)
* [**Getting Started with RStudio**](getting-started.html)

###### Univariate/Descriptive Statistics and Assessing Normality
* [**Univariate/Descriptive Statistics & Normality**](univariate-normality.html) [ <a href="https://raw.githubusercontent.com/burrelvannjr/gradstats_sdsu/main/resources/exercises/univariate-normality.R" target="_blank"><i>R Script</i></a> ]

###### Bivariate and Multivariate Statistics
* [**Independent Samples t-Test**](t-test.html) [ <a href="https://raw.githubusercontent.com/burrelvannjr/gradstats_sdsu/main/resources/exercises/t-test.R" target="_blank"><i>R Script</i></a> | [*Example*](t-test-example.html) ]
* [**One-Way Analysis of Variance (ANOVA)**](anova.html) [ <a href="https://raw.githubusercontent.com/burrelvannjr/gradstats_sdsu/main/resources/exercises/anova.R" target="_blank"><i>R Script</i></a> | [*Example*](anova-example.html) ]
* [**Chi Square Test of Independence**](chi-square.html) [ <a href="https://raw.githubusercontent.com/burrelvannjr/gradstats_sdsu/main/resources/exercises/chi-square.R" target="_blank"><i>R Script</i></a> | [*Example*](chi-square-example.html) ]
* [**Correlation**](correlation.html) [ <a href="https://raw.githubusercontent.com/burrelvannjr/gradstats_sdsu/main/resources/exercises/correlation.R" target="_blank"><i>R Script</i></a> | [*Example*](correlation-example.html) ]
* [**Regression**](regression.html) [ <a href="https://raw.githubusercontent.com/burrelvannjr/gradstats_sdsu/main/resources/exercises/regression.R" target="_blank"><i>R Script</i></a> | [*Example*](regression-example.html) ]
<br><br>

#### Are there mean differences in miles-per-gallon (<span>`mpg`</span>) by type of cylinders in a car (<span>`cyl`</span>)?

Here, we'll be working from the <span style="color:blue">`mtcars`</span> data set, to examine mean differences in the miles per gallon of a car (<span>`mpg`</span>: measured as an interval-ratio variable) by the type of car, in its number of cylinders (<span>`cyl`</span>: measured 4 for four cylinder cars, 6 for six cylinder cars, and 8 for eight cylinder cars).

<br>


### What is the Analysis of Variance (ANOVA)?

The ANOVA test examines the differences in means between **three or more** groups, in effort to see if the differences reflect true differences that we could expect to find in the population. The resulting test calculates an F value.  


<br>

#### Load the Necessary Stuff

```{r, echo=F}
options(repos=c(CRAN="http://cran.wustl.edu/"))
```

```{r, results="hide", warning=FALSE, message=FALSE}
library(MASS)
library(psych)
library(vannstats)
```
<br>

#### Reading in the Data
```{r}
data1 <- mtcars
```
<br>

### Assumptions and Diagnostics for the One-Way ANOVA

The assumptions for an ANOVA are...

* Independence of Observations
* Equal Sample Sizes
* Homogeneity of Variance
* Normality


##### 1. Independence of Observations (Examine Data Collection Strategy)
* Groups are not related or dependent upon each other. Case can’t be in more than one group. No ties between observations. Examine data collection strategy to see if there are linkages between observations. 

  + <span style="color:red">Given that the <span>`mtcars`</span> data have been randomly-sampled, we have met the assumption of independence of observations.</span>

<br>

##### 2. Equal Sample Sizes (Examine N for each group)
* The number of cases in each group should be relatively similar. (If not, use pooled variance/unequal variances asssume t-test formula)

##### 3. Homogeneity of Variance (Examine SD^2^ for each group)
* All groups have approximately equal variances (SD^2^). The distributions (or spread) for the groups are approximately equal. Keppel & Zedeck (1989) suggest that variance comparison should not exceed 10:1 ratio (*or... alternatively, the SDs, when compared, should not exceed around a 3:1 ratio*). 

For both of the above assumptions, we can examine the univariate data table, broken out by group:

```{r}
describeBy(data1$mpg, data1$cyl)
```

  + <span style="color:red">Given that the group sizes are similar, `we have met the assumption of equal sample sizes`. Further, given that the standard deviations for all three groups do not exceed a 3:1 ratio, `we have met the assumption of homogeneity of variance`.</span>

<br>

##### 4. Normality (Examine Plots: Histogram, Q-Q Normality Plots, Box-and-Whiskers Plots)
* Distribution must be relatively normal. (If violated, use “unequal variances assumed” formula, otherwise, use “equal variances assumed”)



##### 4a. Histogram

Plot the histogram for mpg (Y variable) broken out by number of cylinders in a car (levels of the X variable)...  

```{r, warning=FALSE}
hst(data1, mpg, cyl)
```

  + <span style="color:red">We can see from the histograms that the distributions of the outcome variable (mpg) by the predictor/grouping/independent variable (cyl), are relatively normal. Interestingly, the missing right tail of the distribution for 4-cylinder cars implies a longer right tail, which would indicate positive skewness. Yet, overall, these data are close enough to normal.</span> 

<br>

##### 4b. Boxplots (Box-and-Whisker Plots)

Boxplots also provide a visual representation of the normality of a distribution. The boxplot has a box, a line through the box, two whiskers on either end of the box, and sometimes dots/points outside the whiskers. Below, we get a sense of what each part of the boxplot represents...

+ Bottom (or left end) of the **whisker** represents the minimum score for that variable's distribution
+ Bottom (or left end) of the **box** represents the first quartile (the 25th percentile case)
+ Middle line (or dot) inside the **box** represents the median, also known as the second quartile (the 50th percentile case)
+ Top (or right end) of the **box** represents the third quartile (the 75th percentile case)
+ Top (or right end) of the **whisker** represents the maximum score for that variable's distribution
+ Outside dots represent outliers - extreme high or extreme low values for that variable. 

#
#

To tell if a variable is normally-distrubted using the box-and-whisker plot, generally, we want to see that there is *some* distance between the box and the end of the whiskers, that the box isn't pushed too close to either whisker, that the median line (dot) is near the center of the box, and that there aren't many outliers (dots) on the outside of the whiskers.

#


To plot a boxplot, broken out by *Number of Cylinders in a Car*, we can do the following...


```{r}
box(data1, mpg, cyl)
```

  + <span style="color:red">We can see from the boxplots that the data for all three groups tend to be normally-distributed: All medians fall in the center of the interquartile range and that interquartile range is generally centered between the whiskers. The data for 8-cylinder cars, however, possesses an outlier. These plots may give us some pause in proceeding. However, the data seem *normal enough*. It is safe to assume that these data are close enough to normal, since they aren't *drastically* different from normal, and therefore safe to proceed with the statistical test.</span> 

<br>

##### 4c. Normal Q-Q (Quantile-Quantile) Plots

The quantile-quantile plot is a visual tool to help us figure out if the empirical distribution of our variable fits (or rather, comes from) a theoretical normal distribution.

We assess normality an break this plot out by a grouping variable. 

```{r}
qq(data1, mpg, cyl)
```

  + <span style="color:red">We can see from the Q-Q plot that group distributions of the outcome variable (mpg), the data are somewhat normal. However, it is important to notice that for the 4-cylinder and the 8-cylinder cars, the data tend to curve *away from* the normality line at the tails of the distribution. This indicates *some* deviation from normality. We would want to possibly increase our sample size, since these deviations from normality are likely resulting from the small sample size ($n$). Given that the data are normal enough, and there is no discernible pattern across the line (e.g. no strong curvilinear trend around normality line) for the *mpg* variable for any group/level (*cyl*), it is safe to proceed with the statistical test.</span> 

<br>

  + <span style="color:red">Across all three plots of <span>`mpg`</span> broken out by <span>`cylinder`</span>, the variables do not seem to drastically deviate from normality. Therefore, `we can assume normality`.</span>
  
<br>

### The One-Way ANOVA (F-Test) Calculation

The calculation for the F-Test is:

 $F = \frac{{MS}_{between}}{{MS}_{within}} = \frac{\frac{{SS}_{between}}{df_{between}}}{\frac{{SS}_{within}}{df_{within}}}$
 
where... <br>

* ${MS}_{between}$ is the mean square for the treatment, effect, or between groups<br>
* ${MS}_{within}$ is the mean square for the error, or within groups<br>
* ${SS}_{between} = \sum n_{group}(\bar{X}_{group} - \bar{X}_{total})^2$ is the sum of squares for the treatment, effect, or between groups; where $\bar{X}_{total}$ is the grand mean, or the mean of means<br>
* ${SS}_{within} = \sum (X - \bar{X}_{group})^2$ is the square for the error, or within groups<br>

In addition, the degrees of freedom ($df$) for the test is...<br> 
$df_{between} = k - 1$; where $k$ is the number of groups
$df_{within} = N - k$

<br>

### Running the One-Way ANOVA

To run the one-way ANOVA in R, we can use the <span style="color:blue">`ow.anova`</span> function from the `vannstats` package.

For the One-Way ANOVA, within the <span style="color:blue">`ow.anova`</span> function, the data set is listed first, followed by the dependent (interval-ratio level) variable, and the independent (categorical) variable is listed second. 

Additionally, within the <span style="color:blue">`ow.anova`</span> function, you have the option to request a means plot (by adding the call <span style="color:blue">`plot = T`</span>), and you also have the option of requesting a Tukey's HSD post-hoc comparisons test (by adding the call <span style="color:blue">`hsd = T`</span>). I have added both below


```{r}
ow.anova(data1, mpg, cyl, plot = T, hsd = T)
```

In the output above, we see the F-obtained value (39.7), the degrees of freedom between and within (2,29), and the p-value (4.98e-09, or .00000000498, which is much less than our set alpha level of .05).

To interpret the findings, we report the following information:

* The test used
* If you **reject** or **fail to reject** the null hypothesis
* The variables used in the analysis
* The degrees of freedom, calculated value of the test ($F_{obtained}$), and $p-value$
  + $F(df_{between},df_{within}) = F_{obtained}$, $p-value$

“Using a one-way ANOVA, I reject/fail to reject the null hypothesis that there is no mean difference between groups, in the population, $F(?) = ?, p ? .05$” 

  + <span style="color:red">“Using one-way ANOVA, I reject the null hypothesis that there is no mean difference between the miles per gallon in cars with different cylinders, in the population, $F(2,29) = 39.7, p \leq .05$”</span>

<br><br>

### Post-Hoc Checks: Which means differ?

After finding a significant result in your omnibus/overall F-test/ANOVA, to identify *where* the differences lie, you can do two things:

* Examine a means plot
* Run a Post-hoc significance test

#### Means Plot

The means plot can be called from the <span style="color:blue">`ow.anova`</span> function. As seen above: 

  + <span style="color:red">Here, we can see that it looks like 4-cylinder cars have extremely different (higher) mean mpgs than with 6-cylinder or 8-cylinder cars.</span>

<br>

#### Post-Hoc Significance Test: Tukey's HSD

And finally, we can see where the *significantly different* mean comparisons are, with the Tukey's HSD test... which can also be called from the <span style="color:blue">`ow.anova`</span> function. As seen above:

  + <span style="color:red">Here, we see that 4-cylinder cars are, actually, significantly different than 6-cylinder or 8-cylinder cars, and that 6-cylinder cars are, themselves, significantly different from 8-cylinder cars.</span>


<br><br><br>
