---
title: "OLS Regression"
#author: "Burrel Vann Jr"
output: 
  html_document: 
    css: markdown3.css
---
<br>

###### Getting RStudio Up and Running
* [**Installing R/RStudio**](installing.html)
* [**Getting Started with RStudio**](getting-started.html)

###### Univariate/Descriptive Statistics and Assessing Normality
* [**Univariate/Descriptive Statistics & Normality**](univariate-normality.html) [ <a href="https://raw.githubusercontent.com/burrelvannjr/gradstats_sdsu/main/resources/exercises/univariate-normality.R" target="_blank"><i>R Script</i></a> ]

###### Bivariate and Multivariate Statistics
* [**Independent Samples t-Test**](t-test.html) [ <a href="https://raw.githubusercontent.com/burrelvannjr/gradstats_sdsu/main/resources/exercises/t-test.R" target="_blank"><i>R Script</i></a> | [*Example*](t-test-example.html) ]
* [**One-Way Analysis of Variance (ANOVA)**](anova.html) [ <a href="https://raw.githubusercontent.com/burrelvannjr/gradstats_sdsu/main/resources/exercises/anova.R" target="_blank"><i>R Script</i></a> | [*Example*](anova-example.html) ]
* [**Chi Square Test of Independence**](chi-square.html) [ <a href="https://raw.githubusercontent.com/burrelvannjr/gradstats_sdsu/main/resources/exercises/chi-square.R" target="_blank"><i>R Script</i></a> | [*Example*](chi-square-example.html) ]
* [**Correlation**](correlation.html) [ <a href="https://raw.githubusercontent.com/burrelvannjr/gradstats_sdsu/main/resources/exercises/correlation.R" target="_blank"><i>R Script</i></a> | [*Example*](correlation-example.html) ]
* [**Regression**](regression.html) [ <a href="https://raw.githubusercontent.com/burrelvannjr/gradstats_sdsu/main/resources/exercises/regression.R" target="_blank"><i>R Script</i></a> | [*Example*](regression-example.html) ]
<br><br>



#### Can we predict/explain variation in car fuel economy (<span>`mpg`</span>) from the combination of its weight (<span>`wt`</span>) and whether it has a manual or automatic transmission (<span>`am`</span>)?

Here, we'll be working from the <span style="color:blue">`mtcars`</span> data set, to examine the relationship between a car's weight (<span>`wt`</span>: measured in thousands of pounds) and its fuel economy (<span>`mpg`</span>: measured in miles per gallon).

<br>





### What is the Regression?

The OLS regression examines the predictive relationship between some independent variable(s), and an interval-ratio dependent variable. The test tells us about the effect (slope) of any independent (X) variable on an interval-ratio dependent (Y) variable. In particular, the regression equation looks at how values of an x variable "predict" a specific Y value.


Here, we'll look at a multiple (multivariate OLS) regression for the mtcars data, with miles per gallon (mpg) as the DV, car weigh (wt) as IV1 and whether or not the car has an automatic transmission (at) as IV2.

<br>



#### Load the Necessary Stuff

```{r, echo=F}
options(repos=c(CRAN="http://cran.wustl.edu/"))
```

```{r, results="hide", warning=FALSE, message=FALSE}
library(MASS)
library(psych)
library(vannstats)
```


<br>

#### Reading in the Data
```{r}
data1 <- mtcars
```

<br>

#### Assumptions and Diagnostics for Regression

The assumptions for the regression are...

* Adequate Sample Size
* Absence of Outliers
* Absence of Multicollinearity and Singluarity
* Linearity, Normality, and Homoscedasticity (Homogeneity of Variance)

In addition, the previously-discussed assumptions for other tests (independence of observations) is implied, since all of these bivariate tests require random samples. Beyond this, the OLS regression requires an interval-ratio outcome variable. 


##### 1. Adequate Sample Size

* According to Green (1991), as cited in Tabachnick and Fidel (2006), adequate sample size is determined by the modified equation $N \geq 50 + 8(k)$

Where $k$ is the number of independent variables included in the regression model. 


  + <span style="color:red">Given that we have two IVs/predictor variables, the minimum number of cases to be adequate is 66 ($66 = 50 + 8(2)$). Therefore, with only 32 observations in the `mtcars` data set, we do not have enough cases to adequately run the regression model. That is, `we have violated (not met) the assumption of adequate sample size`. I would advise not proceeding with the regression model, however, given that this is an example, I will proceed.</span>

<br>

##### 2. Absence of Outliers

To identify outliers, simply look at the *boxplots* for each variable in the model (Y and all Xs) to see "how outlying, these outliers are." In most cases, outliers should remain in the data. Need strong justification for removing outlying cases.

```{r}
box(data1, mpg)
```

```{r}
box(data1, wt)
```

```{r}
box(data1, am)
```

  + <span style="color:red">We can see from the boxplots that the distributions of the variables are relatively normal. Interestingly, the boxplot for the weight variable has some issues: the median is closer to the 75th percentile, and the upper whisker (right tail) of the distribution for weight variable has some outliers, implying a longer right tail. While we might consider removing these outlying cases, we would need to do so statistically (considering *how* outlying an outlier is)... which is beyond the scope of this class. Moreover, because there are so few cases in the data set ($N = 32$), I would not consider removing cases, as doing so would drastically alter estimation (e.g. $\bar{X}$, etc.). The boxplot for the 'am' variable (whether or not a car has a manual transmission) has some issues. This is to be expected: because the variable is measured as 0 (automatic) and 1 (manual), cases can only be the values 0 or 1 -- nothing in between. This is why we see the median line at the bottom of the box, and why there are no whiskers. Keep this in mind for when you have dichotomous/categorical variables in a regression model. Taken together, `these data represent a relative absence of outliers`.</span> 
  
<br>

##### 3. Multicollinearity and Singularity
* Multicollinearity: Independent variables (more) highly correlated with one another (compared to their correlation with the DV).

  + Check the correlation matrix for variables.

```{r}
cormat(data1, mpg ~ wt + am)
```

  + <span style="color:red">We can see from the correlation matrix that none of the bivariate relationships between the independent variables (`wt` and `am`) variables are *above* a correlation coefficient of $r \approx .90$. Therefore, `we have met the assumption of absence of multicollinearity`.</span> 
  
<br>

* Singularity: If independent variables included are (together) all possible subsets of measure also included in model. For example, if you have a xenophobia scale... based on 4 different questions (the sum of the scale is a "total xenophobia" scale)... and you include all 4 questions in the regression model, AND you include the total scale (the sum of the 4 questions) in the model. There will be so much overlap in the total scale, and the 4 items, that all of them would appear in the regression model with no coefficients... no $b$ values... 

  + Look at the items and determine if they are subsets of other items also included. 

<br>

  + <span style="color:red">Based on the data, `wt` and `am` are not subsets of one another. Therefore, `we have met the assumption of absence of singularity`.</span> 
  
<br>
  

##### 1. Linearity, Normality, and Homoskedasticity
* Linearity: Variables move together in a linear fashion.
* Normality: Variables are normally-distributed.
* Homoskedasticity: Homogeneity of Variance - Variance of variables are similar (10:1, 3:1 for SDs).
  + Visual inspection of **Residuals Plot** to see if relationship is linear, normal, and similar variances. Plot should have **points that extend beyond both sides of the 0 line** (normality), **should not have a U or inverted-U shape in the points** (linearity), and it **should not have a funnel shape**, where points are tightly clustered near the 0 line at one end of the plot, and completely dispersed along y-axis at other end of plot (homoskedasticity).


```{r}
residplot(data1, mpg ~ wt + am)
```


  + <span style="color:red">Based on the residuals plot (the difference between the actual $Y$ and the $\hat{Y}$), we see that `we have met the assumptions of linearity, normality, and homoskedasticity`. Linearity is met given that the residuals do not exhibit a non-linear (e.g. curvilinear) relationship about the 0 distance (from $\hat{Y}$) line. Normality is met given that the residuals do not have a hard stop on either side of the line -- that is, they are evenly distributed about the 0 distance (from $\hat{Y}$) line. Finally, homoskedasticity is met given that the residuals are evenly distanced from the 0 distance (from $\hat{Y}$) line at all values of $\hat{Y}$ -- as exemplified the lack of "fanning out" on one end.</span> 


#
### The Regression Calculation

The calculation for the Regression is:

 $\hat{Y} = b_1X_1 + b_2X_2$
 
Where...
* $\hat{Y}$ is the predicted Y value for the combination of slopes for X values
* $b_1$ is the slope associated with $X_1$
* $b_2$ is the slope associated with $X_2$
* $X_1$ is a specific value for the first $X$ variable that you can plug in for a specific case
* $X_2$ is a specific value for the second $X$ variable that you can plug in for a specific case

 





### Running the Regression


For Regression, within the <span style="color:blue">`lm`</span> function, which stands for *linear model*, the dependent variable is listed first and the independent variable is listed second. 


```{r, results="hide"}
lm(mpg ~ wt + am, data=data1)
```

This may seem confusing, so it's best to wrap our <span style="color:blue">`lm`</span> function in a <span style="color:blue">`summary`</span> call... 

```{r}
summary(lm(mpg ~ wt + am, data=data1))
```

To interpret the findings, we report the following information:

* The test used
* The variables used in the full model
* For significant variables, how a variable's slope affects the outcome
* The amount of variance in the outcome explained by the combination of IVs.


  + <span style="color:red">In the output above, using an OLS regression, we see the Y-intercept (or mean MPG) is an MPG value of 37.32. In addition, we see that the b for the wt variable is significant and negatively related to MPG, such that, for every 1-unit (however it is measured) increase in weight of a car, there is a **5.353-unit decrease** in miles per gallon of a car. In addition, whether or not a car has an automatic transmission is unrelated to MPG. <br><br> We also see that this model is significantly better than the null model (with no predictors), as indicated by the omnibus F test: $F(2,29) = 44.17, p\lt.05$. <br><br> Finally, for this full model, which predicts miles per gallon from the weight of a car whether or not it is automatic transmission, the model fit statistic, the $R^2$, is .7528.  This indicates that 75.28 percent of the variation in a car's fuel economy (`mpg`) is explained by the combination of it's weight (`wt`) and whether or not it runs on a manual transmission (`am`). </span>


<br><br><br>
