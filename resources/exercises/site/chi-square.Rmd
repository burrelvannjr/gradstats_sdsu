---
title: "Chi-Square Test of Independence"
---
---
[ <a href="https://raw.githubusercontent.com/burrelvannjr/gradstats_sdsu/main/resources/exercises/chi-square.R" target="_blank"><i>R Script</i></a> | [*Example*](chi-square-example.html) ]


#### Is there a relationship between the shape of a car's engine (<span>`vs`</span>) and its transmission type (<span>`am`</span>)?

Here, we'll be working from the <span style="color:blue">`mtcars`</span> data set, to examine the relationship between the shape of a car's engine (<span>`vs`</span>: measured 0 for V-shaped engines and 1 for straight-shaped engines) and its transmission type (<span>`am`</span>: measured 0 for automatic and 1 for manual).

<br>


### What is the Chi Square Test of Independence?

The Chi Square test ($X^2$) examines the association or relationship between two nominal/ordinal variables to see if the relationship reflects a true relationship that we could expect to find in the population. The test also tells us whether or not a category (attribute) of one variable varies by categories of another variable. 

The test is called the *test of independence* because it really tests the absence of association between (independence of) two variables.

<br>



#### Load the Necessary Stuff

```{r, echo=F}
options(repos=c(CRAN="http://cran.wustl.edu/"))
```

```{r, results="hide", warning=FALSE, message=FALSE}
library(MASS)
library(psych)
library(vannstats)
```
<br>

#### Reading in the Data
```{r}
data1 <- mtcars
```

<br>

### Assumptions and Diagnostics for Chi Square

The assumptions for the Chi Square are...

* Independence of Observations
* Normality




##### 1. Independence of Observations (Examine Data Collection Strategy)
* Cases (observations) are not related or dependent upon each other. Case can’t have more than one attribute. No ties between observations. Examine data collection strategy to see if there are linkages between observations. 

  + <span style="color:red">Given that the mtcars data have been randomly-sampled, `we have met the assumption of independence of observations`.</span>

<br>

##### 2. Normality (Examine Expected Frequencies Crosstab/Contingency Table)


* Distributions must be relatively normal.The normality assumption is met if **no more than 20 percent of the cells in our Expected Frequencies crosstab have fewer than 5 cases.** Therefore, if ($E \lt 5$) for more than 20 percent of the cells in the Expected Frequencies table, the assumption of normality is violated. If the assmuption of normality is violated, the researcher can employ Yates' Continuity Correction, which conservatively adjusts the numerator for small sample sizes. To employ this correction in R, simply change the option to `correct = TRUE`.

In the <span style="color:blue">`vannstats`</span> package, I have included the <span style="color:blue">`tab`</span> function which returns the crosstabs of observed and expected frequencies. To check if you've met the assumption of normality (e.g. fewer than 20% of cells in the crosstab of expected frequencies falls below $n=5$), you use the following:

```{r}
tab(data1, vs, am)
```

  + <span style="color:red">We see here that `we have met the assumption of normality`: less than 20% of cells in the 2x2 Expected Frequency crosstab have fewer than 5 expected counts. Actually, no cell has fewer than 5.</span>


```{r, results="hide", echo=FALSE}
table(data1$vs, data1$am)
```

<br>

### The Chi Square Test Calculation

The calculation for the Chi Square is:

 $X^2 = \sum \frac{(f_o - f_e)^2}{f_e}$ or
 $X^2 = \sum \frac{(f_{o_i} - f_{e_i})^2}{f_{e_i}}$
 
where... <br>

* $f_o$ (or $f_{o_i}$) is the observed frequency for that cell (the $i^{th}$cell)<br>
* $f_e$ (or $f_{e_i}$) is the expected frequency for that cell (the $i^{th}$cell)<br>
  + $f_e$ (or $f_{e_i}$) for each cell is calculated by the following:
    + $f_e$ (or $f_{e_i}$) $= \frac{(r_{total_i})(c_{total_i})}{total}$ $= \frac{(n_{row_i})(n_{column_i})}{N}$


In addition, the degrees of freedom ($df$) for the test is...<br> 
* $df = (r-1)(c-1)$

where...

* $r$ is the number of rows in a crosstabulation<br>
* $c$ is the number of columns in a crosstabulation<br>


<br>

### Running the Chi Square Test

For Chi Square, within the <span style="color:blue">`chi.sq`</span> function, the dependent variable is listed first and the independent variable is listed second. 


```{r}
#chisq.test(data1$vs, data1$am, correct=FALSE)
chi <- chi.sq(data1, vs, am)
summary(chi)
```

In the output above, we see the $X^2$-obtained value (0.90688), the degrees of freedom (1), and the p-value (0.3409, which is greater than our set alpha level of .05).

To interpret the findings, we report the following information:

* The test used
* If you **reject** or **fail to reject** the null hypothesis
* The variables used in the analysis
* The degrees of freedom, calculated value of the test ($X^2_{obtained}$), and $p-value$
  + $X^2(df) = X^2_{obtained}$, $p-value$

“Using the Chi Square test of independence ($X^2$), I reject/fail to reject the null hypothesis that there is no association between variable one and variable 2, in the population, $X^2(?) = ?, p ? .05$” 

  + <span style="color:red">“Using the Chi Square test of independence ($X^2$), I fail to reject the null hypothesis that there is no association between the shape and the transmission type of cars, in the population, $X^2(1) = .90688, p \gt .05$” </span>


<br>

#### Yates' Continuity Correction for the Chi Square Test 

The calculation for Yates' Chi Square is:

 $X^{2}_{Yates'} = \sum \frac{(|f_o - f_e| - 0.5)^2}{f_e}$    or   
 $X^{2}_{Yates'} = \sum \frac{(|f_{o_i} - f_{e_i}| - 0.5)^2}{f_{e_i}}$

To employ Yates' Continuity Correction, when we have violated the assumption of normality, simply update the `chi.sq` option to `correct = TRUE`, as below

<br>

```{r, results="hide", warning=FALSE, message=FALSE}
#chisq.test(data1$vs, data1$am, correct=TRUE)
chi <- chi.sq(data1, vs, am, post = T)
summary(chi)
```

<br><br>

### Post-Hoc Checks: Which cells differ?

After finding a significant result in your omnibus/overall chi square test, to identify *where* the differences lie, you can do one thing:

* Run a Post-hoc significance test, using 


```{r}
chi <- chi.sq(data1, vs, am, post = T)
summary(chi)
```

<br>

#### Post-Hoc Significance Test

And finally, we can see where the *significantly different* different comparisons (between observed and expected) are, Bonferroni's adjusted p-values  which can also be called from the <span style="color:blue">`chi.sq`</span> function. As seen above:

  + <span style="color:red">Here, we see that none of the comparisons -- between observed and expected cells -- significantly differ.</span>


<br><br><br>


<br><br><br>
